{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e022b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import parsers\n",
    "import random\n",
    "import torch\n",
    "import plotly.express as px\n",
    "from typing import Dict\n",
    "from model import TextMappingModel\n",
    "from config import Config\n",
    "from constants import SPECIAL_TOKENS\n",
    "from transformers import AutoTokenizer\n",
    "from data import LPMappingDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import generate_decoder_inputs_outputs\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6a3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "RES_FILEPATH = 'results/best-checkpoint/test.out.json'\n",
    "CKPT_PATH = 'best-checkpoint.mdl'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model(model_dir):\n",
    "    print(f'Loading model from {model_dir}')\n",
    "    saved_dict = torch.load(os.path.join(model_dir, CKPT_PATH), map_location = DEVICE)\n",
    "    config = Config.from_dict(saved_dict['config'])\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.bert_model_name)\n",
    "    tokenizer.add_tokens(SPECIAL_TOKENS)\n",
    "\n",
    "    model = TextMappingModel(config)\n",
    "    model.load_bert(config.bert_model_name)\n",
    "    model.bert.resize_token_embeddings(len(tokenizer))\n",
    "    model.load_state_dict(saved_dict['model'])\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    print(f'Loading dataset for {model_dir}')\n",
    "    dataset = LPMappingDataset(\n",
    "        path = '../data/test.jsonl',\n",
    "        tokenizer = tokenizer,\n",
    "        max_length = config.max_length,\n",
    "        gpu = torch.cuda.is_available(),\n",
    "        enrich_ner = config.enrich_ner,\n",
    "    )\n",
    "    dataset.numberize()\n",
    "    dataloader = DataLoader(dataset, batch_size = 1, shuffle = False, collate_fn = dataset.collate_fn)\n",
    "\n",
    "    return model, dataloader, config, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df422a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_output(model, batch, tokenizer, config):\n",
    "    decoder_inputs_outputs = generate_decoder_inputs_outputs(\n",
    "        batch,\n",
    "        tokenizer,\n",
    "        model,\n",
    "        torch.cuda.is_available(),\n",
    "        config.max_position_embeddings,\n",
    "        replace_pad_tokens = False,\n",
    "    )\n",
    "    decoder_input_ids = decoder_inputs_outputs['decoder_input_ids']\n",
    "    decoder_labels = decoder_inputs_outputs['decoder_labels']\n",
    "    \n",
    "    model(batch, decoder_input_ids, decoder_labels, tokenizer = tokenizer)\n",
    "    return decoder_labels, model.encode(batch, decoder_input_ids, decoder_labels)\n",
    "\n",
    "\n",
    "def plot_cross_attention(output, input_ids, tokenizer, title, label_ids = None):\n",
    "    last_cross_attention = output['cross_attentions'][-1]\n",
    "    assert last_cross_attention.size(0) == 1, f'Expected one example but found {last_cross_attention.size(0)}'\n",
    "    assert input_ids.size(0) == 1, f'Expected one example but found {input_ids.size(0)}'\n",
    "    assert label_ids is None or label_ids.size(0) == 1, f'Expected one example but found {label_ids.size(0)}'\n",
    "    \n",
    "    input_ids = input_ids[0]\n",
    "    if label_ids is None:\n",
    "        output_logits = output['logits'][0]\n",
    "        label_ids = output_logits.argmax(dim = -1)\n",
    "    else:\n",
    "        label_ids = label_ids[0]\n",
    "    \n",
    "    last_cross_attention = last_cross_attention[0]\n",
    "    # mean across heads\n",
    "    last_cross_attention = last_cross_attention.mean(dim = 0)\n",
    "    assert last_cross_attention.size(0) == label_ids.size(0)\n",
    "    assert last_cross_attention.size(1) == input_ids.size(0)\n",
    "    \n",
    "    last_cross_attention = last_cross_attention.cpu().numpy()\n",
    "    inputs = [tokenizer.decode(i) for i in input_ids]\n",
    "    labels = [tokenizer.decode(i) for i in label_ids]\n",
    "    \n",
    "    fig_width = len(inputs) * 10\n",
    "    fig_height = len(labels) * 10\n",
    "    fig = px.imshow(\n",
    "        last_cross_attention,\n",
    "        width = fig_width,\n",
    "        height = fig_height,\n",
    "        aspect = 'auto',\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title = title,\n",
    "        coloraxis_colorbar = dict(\n",
    "            len = fig_height * 0.93,\n",
    "            lenmode = 'pixels'\n",
    "        ),\n",
    "    )\n",
    "    fig.update_xaxes(\n",
    "        tickangle = 270,\n",
    "        tickfont = dict(family = 'Rockwell', size = 8),\n",
    "        tickmode = 'array',\n",
    "        tickvals = list(range(len(inputs))),\n",
    "        ticktext = inputs,\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        tickfont = dict(family = 'Rockwell', size = 8),\n",
    "        tickmode = 'array',\n",
    "        tickvals = list(range(len(labels))),\n",
    "        ticktext = labels,\n",
    "    )\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5d6a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_model_dir = '../output/default/20230514_151250415'\n",
    "default_noner_model_dir = '../output/default_noner/20230515_223104883'\n",
    "\n",
    "ner_model, ner_dataloader, ner_config, ner_tokenizer = load_model(default_model_dir)\n",
    "noner_model, noner_dataloader, noner_config, noner_tokenizer = load_model(default_noner_model_dir)\n",
    "\n",
    "assert ner_config.bert_model_name == noner_config.bert_model_name\n",
    "\n",
    "ner_data_iter = iter(ner_dataloader)\n",
    "noner_data_iter = iter(noner_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882a3a9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_next(save = False, output_dir = None):\n",
    "    ner_batch = next(ner_data_iter)\n",
    "    noner_batch = next(noner_data_iter)\n",
    "    assert ner_batch.doc_ids[0] == noner_batch.doc_ids[0]\n",
    "\n",
    "    ner_labels, ner_output = get_output(\n",
    "        model = ner_model,\n",
    "        batch = ner_batch,\n",
    "        tokenizer = ner_tokenizer,\n",
    "        config = ner_config,\n",
    "    )\n",
    "    noner_labels, noner_output = get_output(\n",
    "        model = noner_model,\n",
    "        batch = noner_batch,\n",
    "        tokenizer = noner_tokenizer,\n",
    "        config = noner_config,\n",
    "    )\n",
    "\n",
    "    assert (ner_labels == noner_labels).all()\n",
    "\n",
    "    ner_fig = plot_cross_attention(\n",
    "        output = ner_output,\n",
    "        input_ids = ner_batch.input_ids,\n",
    "        tokenizer = ner_tokenizer,\n",
    "        title = 'NER Augmentation',\n",
    "        label_ids = ner_labels,\n",
    "    )\n",
    "    noner_fig = plot_cross_attention(\n",
    "        output = noner_output,\n",
    "        input_ids = noner_batch.input_ids,\n",
    "        tokenizer = noner_tokenizer,\n",
    "        title = 'No NER Augmentation',\n",
    "        label_ids = noner_labels,\n",
    "    )\n",
    "    \n",
    "    ner_fig.show()\n",
    "    noner_fig.show()\n",
    "    \n",
    "    if save:\n",
    "        assert output_dir is not None, f'output_dir is required to save the plots'\n",
    "        \n",
    "        ner_fig.write_image(os.path.join(output_dir, 'ner.png'))\n",
    "        noner_fig.write_image(os.path.join(output_dir, 'noner.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb55f020",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_next(save = True, output_dir = '../output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02adb64e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
